{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, time, json, sys\n",
    "import pickle\n",
    "\n",
    "import itertools, sys, math, time\n",
    "from scipy.spatial import distance as dist\n",
    "from numpy import linalg\n",
    "\n",
    "from matplotlib import offsetbox\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import cluster, datasets, decomposition, ensemble, lda, manifold, random_projection\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from scipy import sparse as sp\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_where(sparse_matrix, num):\n",
    "    \"\"\"\n",
    "    np.where() for a sparse matrix. Returns a set of indices.\n",
    "    \"\"\"\n",
    "    return set(np.where(sparse_matrix[num,:].toarray())[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_data(filename, num_lines):\n",
    "    \"\"\"\n",
    "    Function to load sparse data.\n",
    "    \"\"\"\n",
    "    inverted_index = collections.defaultdict(set)\n",
    "    \n",
    "    sparse_indptr = [0]\n",
    "    sparse_indices = []\n",
    "    sparse_data = []\n",
    "    vocabulary = {}\n",
    "\n",
    "    print 'Reading data.'\n",
    "    for line_num, line in enumerate(open(filename)):\n",
    "        new_row = [(idx,float(prob)) for idx, prob in enumerate(line.strip().split(',')) if float(prob) > 0.0]\n",
    "        for i,p in new_row:\n",
    "            sparse_indices.append(i)\n",
    "            sparse_data.append(p)\n",
    "            inverted_index[i].add(line_num)\n",
    "        sparse_indptr.append(len(sparse_indices))\n",
    "        sys.stdout.write(\"\\r%d%%\" % (100.0 * line_num / num_lines))\n",
    "        sys.stdout.flush()\n",
    "    print 100.0 * line_num / num_lines, '%'\n",
    "    print 'Done reading data.'\n",
    "\n",
    "    sparse_matrix = csr_matrix((sparse_data, sparse_indices, sparse_indptr), dtype=float)\n",
    "    return sparse_matrix, inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Skip parts that take a long time.\n",
    "SKIP_LONG_PARTS = True\n",
    "\n",
    "# Create a dense representation of the data.\n",
    "CREATE_DENSE_ARRAY = False\n",
    "\n",
    "NUM_SPEECHES = 2740\n",
    "\n",
    "NUM_DEBATES = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data.\n",
      "99%99.9635036496 %\n",
      "Done reading data.\n"
     ]
    }
   ],
   "source": [
    "speech_vectors, inverted_index = load_sparse_data('speech_vectors.csv', NUM_SPEECHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data.\n",
      "99%99.9635036496 %\n",
      "Done reading data.\n"
     ]
    }
   ],
   "source": [
    "speech_graph, inverted_graph = load_sparse_data('speech_graph.csv', NUM_SPEECHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None,\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
      "       param_grid={'kernel': ('linear', 'rbf'), 'C': array([ 1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5. ,  5.5,  6. ,\n",
      "        6.5,  7. ,  7.5,  8. ,  8.5,  9. ,  9.5])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
      "       verbose=0)\n",
      "GridSearchCV(cv=None,\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
      "       param_grid={'kernel': ('linear', 'rbf'), 'C': array([ 1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5. ,  5.5,  6. ,\n",
      "        6.5,  7. ,  7.5,  8. ,  8.5,  9. ,  9.5])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
      "       verbose=0)\n",
      "0.528832116788\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, grid_search\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from scipy import sparse as sp\n",
    "\n",
    "# run PCA on data\n",
    "truncated_svd = TruncatedSVD(n_components=10)\n",
    "reduced_data = truncated_svd.fit_transform(sp.hstack((speech_graph.todense(), speech_vectors[:, :25000])))\n",
    "\n",
    "# form the training data\n",
    "X = reduced_data[[2, 13, 18, 24, 1, 3, 27, 177], :]\n",
    "y = [0 for i in range(4)] + [1 for i in range(4)]\n",
    "\n",
    "# fit SVM, searching over paramaters\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':np.arange(1, 10, 0.5)}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "a = clf.fit(X, y)\n",
    "print a\n",
    "print clf\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "ones = 0\n",
    "\n",
    "for i in range(len(reduced_data)):\n",
    "    prediction = clf.predict(reduced_data[i])[0]\n",
    "    if prediction == 1:\n",
    "        ones += 1\n",
    "    predictions += [prediction]\n",
    "\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "\n",
    "# print predictions\n",
    "print float(ones)/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99197080292\n"
     ]
    }
   ],
   "source": [
    "good_preds = np.loadtxt(open('preds_laplacian1.csv'),delimiter=\",\",skiprows=1)\n",
    "\n",
    "# score = 0\n",
    "# for i, pred in enumerate(good_preds):\n",
    "#     if pred == predictions[i]:\n",
    "#         score += 1\n",
    "# print 'score' + str(score/2740.0)\n",
    "    \n",
    "print (2740-sum(np.logical_xor(predictions, good_preds[:, 1])))/2740.0\n",
    "\n",
    "with open('svm_pred.csv', 'w') as output_file:\n",
    "    output_file.write('Id,Prediction\\n')\n",
    "    for i, p in enumerate(predictions):\n",
    "        output_file.write(str(i) + ', ' + str(p) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
