{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gensim\n",
    "import heapq\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from matplotlib import offsetbox\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import cluster, datasets, decomposition, ensemble, lda, manifold, random_projection, grid_search\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from scipy import sparse as sp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_evaluation(tuples, filename):\n",
    "    f = open(filename, 'w+')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for t in tuples:\n",
    "        f.write(str(t[0]) + ',' + str(t[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_data(filename, num_lines):\n",
    "    \"\"\"\n",
    "    Function to load sparse data.\n",
    "    \"\"\"\n",
    "    inverted_index = collections.defaultdict(set)\n",
    "    \n",
    "    sparse_indptr = [0]\n",
    "    sparse_indices = []\n",
    "    sparse_data = []\n",
    "    vocabulary = {}\n",
    "\n",
    "    print 'Reading data.'\n",
    "    for line_num, line in enumerate(open(filename)):\n",
    "        new_row = [(idx,float(prob)) for idx, prob in enumerate(line.strip().split(',')) if float(prob) > 0.0]\n",
    "        for i,p in new_row:\n",
    "            sparse_indices.append(i)\n",
    "            sparse_data.append(p)\n",
    "            inverted_index[i].add(line_num)\n",
    "        sparse_indptr.append(len(sparse_indices))\n",
    "\n",
    "        if line_num % 450 == 0:\n",
    "            print 100.0 * line_num / num_lines, '%'\n",
    "    print 100.0 * line_num / num_lines, '%'\n",
    "    print 'Done reading data.'\n",
    "\n",
    "    sparse_matrix = csr_matrix((sparse_data, sparse_indices, sparse_indptr), dtype=float)\n",
    "    return sparse_matrix, inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_where(sparse_matrix, num):\n",
    "    \"\"\"\n",
    "    np.where() for a sparse matrix. Returns a set of indices.\n",
    "    \"\"\"\n",
    "    return set(np.where(sparse_matrix[num,:].toarray())[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_clustering(fname):\n",
    "    cluster_dict = []\n",
    "    cluster_inv_idx = collections.defaultdict(list)\n",
    "    \n",
    "    todo\n",
    "    \n",
    "    return cluster_dict, cluster_inv_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data.\n",
      "0.0 %\n",
      "16.4233576642 %\n",
      "32.8467153285 %\n",
      "49.2700729927 %\n",
      "65.6934306569 %\n",
      "82.1167883212 %\n",
      "98.5401459854 %\n",
      "99.9635036496 %\n",
      "Done reading data.\n",
      "(2740, 2740)\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEECHES = 2740\n",
    "speech_graph, inverted_graph = load_sparse_data('speech_graph.csv', NUM_SPEECHES)\n",
    "print speech_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_label, predicted_label_inv_idx = load_clustering('hacky1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual cluster sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 11, 12, 14, 14, 17, 19, 20, 21, 26, 27, 28, 30, 35, 41, 42, 42, 42, 43, 44, 45, 53, 54, 58, 58, 60, 64, 84, 87, 89, 114, 117, 130, 134, 159, 282, 294, 327]\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "header = False\n",
    "actual_cluster_sizes = []\n",
    "for line in open('challenge-2-label-hint.csv'):\n",
    "    if header:\n",
    "        header = False\n",
    "        continue\n",
    "    actual_cluster_sizes.append(int(line.strip().split(',')[1]))\n",
    "\n",
    "actual_cluster_sizes.sort()\n",
    "print actual_cluster_sizes\n",
    "print len(actual_cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual cluster seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "header = False\n",
    "actual_debates = {}\n",
    "for line in open('actual-debate-labels.csv'):\n",
    "    if header:\n",
    "        header = False\n",
    "        continue\n",
    "    parts = line.strip().split(',')\n",
    "    actual_debates[int(parts[0])] = int(parts[1])\n",
    "    \n",
    "print len(actual_debates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the cluster sizes we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the three largest clusters (9 incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech_graph_copy_trimmed = speech_graph[sorted(remaining),:]\n",
    "print speech_graph_copy_trimmed.shape\n",
    "speech_graph_copy_trimmed = speech_graph_copy_trimmed[:,sorted(remaining)]\n",
    "print speech_graph_copy_trimmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spectral_labels = cluster.spectral_clustering(speech_graph_copy_trimmed, n_clusters=2)\n",
    "plt.hist(spectral_labels, bins=10)\n",
    "print [list(spectral_labels).count(i) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_evaluation([(i, d[i]) for i in range(NUM_SPEECHES)], 'fix.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
